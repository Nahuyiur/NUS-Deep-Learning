{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d36b5dc",
   "metadata": {},
   "source": [
    "# SWS3009 Lab 5B - Pose Estimation with YOLOv7\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this part of Lab 5 we will look at how to do pose estimation with YOLOv7. This part of the lab is just an introduction on how to use the post estimation head in YOLOv7.\n",
    "\n",
    "## 2. Building the Pose Estimator\n",
    "\n",
    "We start off by importing our libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd35edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './yolov7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef89c8",
   "metadata": {},
   "source": [
    "Now we begin by selecting a PyTorch device. Here we are using the CPU, but if you have a CUDA compatible CUDA device, you can instead use device=torch.device(\"cuda\").\n",
    "\n",
    "The load_model function simply calls torch.load to load up the yolov7-w6-pose.pt weights file. The model.float().eval() call sets the dropout parameters correctly to ensure consistent inference.\n",
    "\n",
    "<b>Important:</b> You must have the yolov7-w6-pose.pt weights set, which has been included in your SWS3009Lab5.zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62dd8707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/wch4p99157d31_2xqn8213gr0000gn/T/ipykernel_33858/1533761132.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('yolov7-w6-pose.pt', map_location=device)['model']\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cpu\")\n",
    "\n",
    "def load_model():\n",
    "    global device\n",
    "    model = torch.load('yolov7-w6-pose.pt', map_location=device)['model']\n",
    "    # Turn the model into a float model\n",
    "    model.float().eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c092d2",
   "metadata": {},
   "source": [
    "Our run_inference function takes an image, resizes and pads it to a form suitable for YOLOv7, converts it to an image tensor, then calls the model. Note that we call the model from within \"torch.no_grad\" to prevent updating of the weights.\n",
    "\n",
    "This function then produces a set of \"keypoints\" and the image itself. However the current set of keypoints will contain many duplicates and in draw_keypoints we will use non-maximal suppression to remove most of them. We will do this in the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af773eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(image):\n",
    "    # Resize and pad image. First return value is the resized image\n",
    "    # second is ratio, then dw and dh\n",
    "    # Resize to [567, 960, 3]\n",
    "    image = letterbox(image, 960, stride=64, auto = True)[0]\n",
    "    \n",
    "    # torch.Size([3, 567, 960]). Converts PIL image to tensor\n",
    "    image = transforms.ToTensor()(image)\n",
    "    # Adds an additional dimension of 1 at indicated position\n",
    "    # Turns it into a batch\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    # no_grad disables update of weights\n",
    "    image.to(device)\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "    return output, image\n",
    "\n",
    "# To display images on the web\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e50e88",
   "metadata": {},
   "source": [
    "Our next step is to draw the keypoints. The keypoints are an $n \\times 58$ matrix. I.e. there are $n$ rows of $58$ elements, where $n$ is the number of people detected. Each row of 58 numbers consists of:\n",
    "\n",
    "1. 7 number numbers that represent the batch ID, class ID, x, y, width, height and confidence score of the object detected.\n",
    "2. 17 \"keypoints\" consisting of x, y and confidence values (total is $17 \\times 3 = 51$ values). A \"keypoint\" is a point on the \"skeleton\".  See this diagram for details:\n",
    "\n",
    "![](https://i.stack.imgur.com/HG8dB.png)\n",
    "\n",
    "\n",
    "You can use the keypoint values from output\\[idx, 7:\\] to access the keypoints. Here we just call plot_skeleton_kpts to draw the skeleton. The number of people detected can be found in \"output\\[0\\]\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4368031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image produced from run_inference has many proposals\n",
    "# We run non-maximal suppression to pick the btest\n",
    "\n",
    "def draw_keypoints(output, image):\n",
    "    # 0.25 confidence threshold, 0.65 IoU threshold\n",
    "    # nc = number of classes\n",
    "    output = non_max_suppression_kpt(output, 0.25, 0.65, \n",
    "                                    nc = model.yaml['nc'],\n",
    "                                     nkpt = model.yaml['nkpt'],\n",
    "                                     kpt_label = True)\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "    \n",
    "    # Permute dimensions of tensor\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    \n",
    "    # tensor.cpu() returns copy of tensor in cpu memory\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Convert colorspace from standard RGB to \n",
    "    # CV2 BGR\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Plot the skeleton for each person detected. The number\n",
    "    # of persons detected is in output.shape[0]. The keypoints\n",
    "    # are from index 7 onwards for each person. Index 0 to 6\n",
    "    # are the batch ID, class ID, x, y, width, height and\n",
    "    # confidence score for the object identified.\n",
    "    \n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "    \n",
    "    return nimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74519b",
   "metadata": {},
   "source": [
    "The next function is to capture video from the camera or video file (post estimation is really not as useful in pictures) by calling cv2.VideoCapture. We then read the video frame, call run_inference and draw_keypoints to highlight the pose skeleton. \n",
    "\n",
    "Note that CV2 by default uses a BGR color space instead of RGB, hence we need to coll cvtColor to convert from BGR to RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9fc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a filename of 0 to capture the camera.\n",
    "def pose_estimation_video(filename, outfilename = None):\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    \n",
    "    # Filename, fourcc code, fps, frame dimensions. fourcc code\n",
    "    # specifies the codec\n",
    "    \n",
    "    if outfilename is not None:\n",
    "        # Video writer to capture to MP4q\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(outfilename, fourcc, 30.0, (int(cap.get(3)), \n",
    "                             int(cap.get(4))))\n",
    "    else:\n",
    "        out = None\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        (ret, frame) = cap.read()\n",
    "        if ret == True:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            output, frame = run_inference(frame)\n",
    "            frame = draw_keypoints(output, frame)\n",
    "            frame = cv2.resize(frame, (int(cap.get(3)), int(cap.get(4))))\n",
    "            if out is not None:\n",
    "                out.write(frame)\n",
    "            cv2.imshow('Pose Estimation', frame)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        if cv2.waitKey(15) &  0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    \n",
    "    if out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6410c0",
   "metadata": {},
   "source": [
    "Now finally we call pose_estimation_video to estimate the pose of each person in the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53f8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pose_estimation_video(\"./ice-skating.mp4\")\n",
    "pose_estimation_video(0, outfilename=\"camera.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eaa9b1",
   "metadata": {},
   "source": [
    "## 3. Conclusion\n",
    "\n",
    "This very short lab shows you how to perform pose estimation with YOLO7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94539be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
