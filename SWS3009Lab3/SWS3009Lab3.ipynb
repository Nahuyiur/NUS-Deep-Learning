{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWS3009 Lab 3 Introduction to Deep Learning\n",
    "\n",
    "\n",
    "| Name:    | Liu Yijia                    |\n",
    "|----------|---------------------|\n",
    "| Name:    | Rui Yuhan                    |\n",
    "\n",
    "This lab should be done by both Deep Learning members of the team. Please ensure that you fill in the names of <b>both</b> team members in the spaces above. Answer <b>all</b> your questions on <b>this Python Notebook.</b>\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "### SUBMISSION DEADLINE: July  5 2024, 2359 hours (11.59 pm). Folder will close by 00:15 hours on July 5 afterwhich no submission will be allowed.\n",
    "\n",
    "Please submit this Python notebook to Canvas on the deadline provided.\n",
    "\n",
    "Marks will be awarded as follows:\n",
    "\n",
    "**0 marks**: No/empty/Non-English submission\n",
    "\n",
    "**1 mark** : Poor submission\n",
    "\n",
    "**2 marks**: Acceptable submission\n",
    "\n",
    "**3 marks**: Good submission\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "We will achieve the following objectives in this lab:\n",
    "\n",
    "    1. An understanding of the practical limitations of using dense networks in complex tasks\n",
    "    2. Hands-on experience in building a deep learning neural network to solve a relatively complex task.\n",
    "    \n",
    "\n",
    "Each step may take a long time to run. You and your partner may want to work out how to do things simultaneously, but please do not miss out on any learning opportunities.\n",
    "\n",
    "\n",
    "## 2. Submission Instructions\n",
    "\n",
    "Please submit your answer book to Canvas by the deadline.\n",
    "\n",
    "## 3. Creating a Dense Network for CIFAR-10\n",
    "\n",
    "We will now begin building a neural network for the CIFAR-10 dataset. The CIFAR-10 dataset consists of 50,000 32x32x3 (32x32 pixels, RGB channels) training images and 10,000 testing images (also 32x32x3), divided into the following 10 categories:\n",
    "\n",
    "    1. Airplane\n",
    "    2. Automobile\n",
    "    3. Bird\n",
    "    4. Cat\n",
    "    5. Deer\n",
    "    6. Dog\n",
    "    7. Frog\n",
    "    8. Horse\n",
    "    9. Ship\n",
    "    10. Truck\n",
    "    \n",
    "In the first two parts of this lab we will create a classifier for the CIFAR-10 dataset.\n",
    "\n",
    "### 3.1 Loading the Dataset\n",
    "\n",
    "We begin firstly by creating a Dense neural network for CIFAR-10. The code below shows how we load the CIFAR-10 dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "def load_cifar10():\n",
    "    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
    "    test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
    "    train_x = train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "    ret_train_y = to_categorical(train_y,10)\n",
    "    ret_test_y = to_categorical(test_y, 10)\n",
    "    \n",
    "    return (train_x, ret_train_y), (test_x, ret_test_y)\n",
    "\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = load_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "Explain what the following two  statements do, and where the number \"3072\" came from:\n",
    "\n",
    "```\n",
    "  train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
    "  test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
    "```\n",
    "\n",
    "**Please put your answers in the attached answer books**\n",
    "\n",
    "\n",
    "The two statements reshape each CIFAR-10 image from a 3D array of shape (32, 32, 3) into a 1D vector of size 3072 (=32 × 32 × 3). This is done to prepare the data for input into a neural network model that expects flat input vectors.\n",
    "\n",
    "### 3.2 Building the MLP Classifier\n",
    "\n",
    "In the code box below, create a new fully connected (dense) multilayer perceptron classifier for the CIFAR-10 dataset. To begin with, create a network with one hidden layer of 1024 neurons, using the SGD optimizer. You should output the training and validation accuracy at every epoch, and train for 50 epochs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  8/782 [..............................] - ETA: 5s - loss: 2.3589 - accuracy: 0.1289  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 16:52:38.489356: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777/782 [============================>.] - ETA: 0s - loss: 2.0893 - accuracy: 0.2550"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 16:52:43.809435: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 8ms/step - loss: 2.0882 - accuracy: 0.2555 - val_loss: 1.9926 - val_accuracy: 0.2861\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.9349 - accuracy: 0.3220 - val_loss: 1.9145 - val_accuracy: 0.3250\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.8783 - accuracy: 0.3440 - val_loss: 1.8946 - val_accuracy: 0.3106\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.8483 - accuracy: 0.3554 - val_loss: 1.8372 - val_accuracy: 0.3607\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.8254 - accuracy: 0.3659 - val_loss: 1.8397 - val_accuracy: 0.3419\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.8093 - accuracy: 0.3723 - val_loss: 1.8580 - val_accuracy: 0.3377\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7955 - accuracy: 0.3769 - val_loss: 1.8112 - val_accuracy: 0.3584\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7835 - accuracy: 0.3819 - val_loss: 1.8039 - val_accuracy: 0.3655\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7746 - accuracy: 0.3854 - val_loss: 1.7872 - val_accuracy: 0.3678\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7669 - accuracy: 0.3885 - val_loss: 1.7982 - val_accuracy: 0.3581\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7599 - accuracy: 0.3912 - val_loss: 1.7573 - val_accuracy: 0.3901\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7528 - accuracy: 0.3953 - val_loss: 1.7637 - val_accuracy: 0.3797\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7463 - accuracy: 0.3967 - val_loss: 1.7537 - val_accuracy: 0.3880\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7406 - accuracy: 0.3975 - val_loss: 1.7482 - val_accuracy: 0.3919\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.7352 - accuracy: 0.4009 - val_loss: 1.7952 - val_accuracy: 0.3551\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.7294 - accuracy: 0.4032 - val_loss: 1.7317 - val_accuracy: 0.4014\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.7241 - accuracy: 0.4055 - val_loss: 1.7359 - val_accuracy: 0.3953\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7186 - accuracy: 0.4058 - val_loss: 1.7292 - val_accuracy: 0.3969\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.7139 - accuracy: 0.4089 - val_loss: 1.7305 - val_accuracy: 0.3968\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7086 - accuracy: 0.4129 - val_loss: 1.7384 - val_accuracy: 0.3836\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.7038 - accuracy: 0.4143 - val_loss: 1.7285 - val_accuracy: 0.3838\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6981 - accuracy: 0.4166 - val_loss: 1.7117 - val_accuracy: 0.4049\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6929 - accuracy: 0.4163 - val_loss: 1.7085 - val_accuracy: 0.4078\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6884 - accuracy: 0.4180 - val_loss: 1.7530 - val_accuracy: 0.3669\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6819 - accuracy: 0.4190 - val_loss: 1.6899 - val_accuracy: 0.4117\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6765 - accuracy: 0.4200 - val_loss: 1.7035 - val_accuracy: 0.4090\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.6710 - accuracy: 0.4235 - val_loss: 1.6878 - val_accuracy: 0.4128\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6640 - accuracy: 0.4267 - val_loss: 1.7129 - val_accuracy: 0.3950\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6586 - accuracy: 0.4279 - val_loss: 1.7107 - val_accuracy: 0.3956\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6526 - accuracy: 0.4281 - val_loss: 1.6874 - val_accuracy: 0.4130\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 1.6463 - accuracy: 0.4318 - val_loss: 1.6718 - val_accuracy: 0.4197\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6410 - accuracy: 0.4355 - val_loss: 1.6674 - val_accuracy: 0.4129\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6349 - accuracy: 0.4356 - val_loss: 1.7054 - val_accuracy: 0.4010\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6290 - accuracy: 0.4382 - val_loss: 1.6723 - val_accuracy: 0.4107\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6223 - accuracy: 0.4411 - val_loss: 1.6629 - val_accuracy: 0.4185\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6175 - accuracy: 0.4423 - val_loss: 1.6618 - val_accuracy: 0.4158\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6117 - accuracy: 0.4432 - val_loss: 1.6455 - val_accuracy: 0.4226\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6063 - accuracy: 0.4448 - val_loss: 1.6327 - val_accuracy: 0.4317\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.6004 - accuracy: 0.4479 - val_loss: 1.6418 - val_accuracy: 0.4238\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5954 - accuracy: 0.4494 - val_loss: 1.6529 - val_accuracy: 0.4222\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5910 - accuracy: 0.4500 - val_loss: 1.6122 - val_accuracy: 0.4357\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5860 - accuracy: 0.4510 - val_loss: 1.6063 - val_accuracy: 0.4420\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5809 - accuracy: 0.4546 - val_loss: 1.6349 - val_accuracy: 0.4213\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5758 - accuracy: 0.4568 - val_loss: 1.5994 - val_accuracy: 0.4438\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5712 - accuracy: 0.4573 - val_loss: 1.6104 - val_accuracy: 0.4302\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 1.5680 - accuracy: 0.4585 - val_loss: 1.6027 - val_accuracy: 0.4478\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5627 - accuracy: 0.4598 - val_loss: 1.5911 - val_accuracy: 0.4466\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5594 - accuracy: 0.4618 - val_loss: 1.6089 - val_accuracy: 0.4342\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5554 - accuracy: 0.4643 - val_loss: 1.6206 - val_accuracy: 0.4267\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 1.5506 - accuracy: 0.4652 - val_loss: 1.5896 - val_accuracy: 0.4434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x36fd06160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Write your code to build an MLP with one hidden layer of 1024 neurons,\n",
    "with an SGD optimizer. Train for 50 epochs, and output the training and\n",
    "validation accuracy at each epoch.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(3072,)),\n",
    "    layers.Dense(1024, activation='sigmoid'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_x, train_y, epochs=50,batch_size=64, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Complete the following table on the design choices for your MLP:\n",
    "\n",
    "| Hyperparameter       | What I used                  | Why?                                                                 |\n",
    "|:---------------------|:-----------------------------|:----------------------------------------------------------------------|\n",
    "| Optimizer            | SGD                          | Specified in question                                                |\n",
    "| # of hidden layers   | 1                            | Specified in question                                                |\n",
    "| # of hidden neurons  | 1024                         | Specified in question                                                |\n",
    "| Hid layer activation | Sigmoid                      | A traditional activation function; simple and smooth                 |\n",
    "| # of output neurons  | 10                           | CIFAR-10 has 10 classes                                              |\n",
    "| Output activation    | Softmax                      | Converts outputs to probability distribution for classification      |\n",
    "| learning_rate        | Default                      | Default value used for SGD when not specified                        |\n",
    "| momentum             | Default                      | Not specified in question, so default used                           |\n",
    "| loss                 | Categorical crossentropy     | Because labels are one-hot encoded for multi-class classification    |\n",
    "\n",
    "\n",
    "#### Question 3:\n",
    "\n",
    "What was your final training accuracy? Validation accuracy? Is there overfitting / underfitting? Explain your answer:\n",
    "\n",
    "***PLACE YOUR ANSWER HERE ***\n",
    "\n",
    "Final training accuracy: 0.4652\n",
    "\n",
    "Final validation accuracy: 0.4434\n",
    "\n",
    "Underfitting, for the model shows relatively low accuracy on both training and validation sets, indicating that it hasn’t learned the data patterns well enough. Because MLP model is too simple and the training time is insufficient.\n",
    "\n",
    "### 3.3 Experimenting with the MLP\n",
    "\n",
    "Cut and paste your code from Section 3.2 to the box below (you may need to rename your MLP). Experiment with the number of hidden layers, the number of neurons in each hidden layer, the optimization algorithm, etc. See [Keras Optimizers](https://keras.io/optimizers) for the types of optimizers and their parameters. **Train for 100 epochs.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  6/782 [..............................] - ETA: 9s - loss: 2.7282 - accuracy: 0.1120  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 22:58:14.934597: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 0s - loss: 1.9747 - accuracy: 0.2683"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 22:58:23.977301: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 12ms/step - loss: 1.9747 - accuracy: 0.2683 - val_loss: 1.8225 - val_accuracy: 0.3406\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.8147 - accuracy: 0.3403 - val_loss: 1.7664 - val_accuracy: 0.3637\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.7705 - accuracy: 0.3566 - val_loss: 1.7379 - val_accuracy: 0.3751\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.7447 - accuracy: 0.3694 - val_loss: 1.7552 - val_accuracy: 0.3539\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.7211 - accuracy: 0.3750 - val_loss: 1.6738 - val_accuracy: 0.3983\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.7029 - accuracy: 0.3837 - val_loss: 1.6522 - val_accuracy: 0.4013\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.6875 - accuracy: 0.3862 - val_loss: 1.6857 - val_accuracy: 0.3860\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.6781 - accuracy: 0.3962 - val_loss: 1.6330 - val_accuracy: 0.4210\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.6633 - accuracy: 0.4016 - val_loss: 1.5983 - val_accuracy: 0.4274\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.6524 - accuracy: 0.4049 - val_loss: 1.6049 - val_accuracy: 0.4220\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.6462 - accuracy: 0.4077 - val_loss: 1.6811 - val_accuracy: 0.3792\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.6409 - accuracy: 0.4075 - val_loss: 1.5691 - val_accuracy: 0.4317\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.6288 - accuracy: 0.4109 - val_loss: 1.6188 - val_accuracy: 0.4134\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.6207 - accuracy: 0.4163 - val_loss: 1.5577 - val_accuracy: 0.4348\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.6135 - accuracy: 0.4195 - val_loss: 1.5576 - val_accuracy: 0.4384\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.6056 - accuracy: 0.4206 - val_loss: 1.5625 - val_accuracy: 0.4420\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5980 - accuracy: 0.4259 - val_loss: 1.5499 - val_accuracy: 0.4394\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5957 - accuracy: 0.4245 - val_loss: 1.5809 - val_accuracy: 0.4293\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5933 - accuracy: 0.4263 - val_loss: 1.5496 - val_accuracy: 0.4408\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5884 - accuracy: 0.4274 - val_loss: 1.5127 - val_accuracy: 0.4545\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.5789 - accuracy: 0.4331 - val_loss: 1.5104 - val_accuracy: 0.4543\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5722 - accuracy: 0.4351 - val_loss: 1.5314 - val_accuracy: 0.4483\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5750 - accuracy: 0.4312 - val_loss: 1.5341 - val_accuracy: 0.4494\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5698 - accuracy: 0.4362 - val_loss: 1.5224 - val_accuracy: 0.4550\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5726 - accuracy: 0.4356 - val_loss: 1.5376 - val_accuracy: 0.4429\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5684 - accuracy: 0.4337 - val_loss: 1.5214 - val_accuracy: 0.4416\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5666 - accuracy: 0.4328 - val_loss: 1.5335 - val_accuracy: 0.4427\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5606 - accuracy: 0.4406 - val_loss: 1.5154 - val_accuracy: 0.4548\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5639 - accuracy: 0.4382 - val_loss: 1.5736 - val_accuracy: 0.4377\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5572 - accuracy: 0.4381 - val_loss: 1.4999 - val_accuracy: 0.4580\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5570 - accuracy: 0.4423 - val_loss: 1.5176 - val_accuracy: 0.4566\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5496 - accuracy: 0.4432 - val_loss: 1.4973 - val_accuracy: 0.4626\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5456 - accuracy: 0.4435 - val_loss: 1.5139 - val_accuracy: 0.4572\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5445 - accuracy: 0.4434 - val_loss: 1.5121 - val_accuracy: 0.4557\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5408 - accuracy: 0.4474 - val_loss: 1.5222 - val_accuracy: 0.4499\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5412 - accuracy: 0.4479 - val_loss: 1.4923 - val_accuracy: 0.4618\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5403 - accuracy: 0.4457 - val_loss: 1.4985 - val_accuracy: 0.4626\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5333 - accuracy: 0.4504 - val_loss: 1.4871 - val_accuracy: 0.4666\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5294 - accuracy: 0.4504 - val_loss: 1.4904 - val_accuracy: 0.4647\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5293 - accuracy: 0.4490 - val_loss: 1.4905 - val_accuracy: 0.4649\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5222 - accuracy: 0.4522 - val_loss: 1.4722 - val_accuracy: 0.4748\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5231 - accuracy: 0.4506 - val_loss: 1.4900 - val_accuracy: 0.4622\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5190 - accuracy: 0.4531 - val_loss: 1.4782 - val_accuracy: 0.4720\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5125 - accuracy: 0.4597 - val_loss: 1.5015 - val_accuracy: 0.4632\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5127 - accuracy: 0.4562 - val_loss: 1.4997 - val_accuracy: 0.4640\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5142 - accuracy: 0.4538 - val_loss: 1.4754 - val_accuracy: 0.4689\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5047 - accuracy: 0.4580 - val_loss: 1.4756 - val_accuracy: 0.4714\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5101 - accuracy: 0.4565 - val_loss: 1.4806 - val_accuracy: 0.4703\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5010 - accuracy: 0.4598 - val_loss: 1.4941 - val_accuracy: 0.4660\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5069 - accuracy: 0.4575 - val_loss: 1.4849 - val_accuracy: 0.4712\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4960 - accuracy: 0.4635 - val_loss: 1.4937 - val_accuracy: 0.4701\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4985 - accuracy: 0.4626 - val_loss: 1.4730 - val_accuracy: 0.4709\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4972 - accuracy: 0.4583 - val_loss: 1.4883 - val_accuracy: 0.4661\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4997 - accuracy: 0.4596 - val_loss: 1.4696 - val_accuracy: 0.4735\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4959 - accuracy: 0.4634 - val_loss: 1.4686 - val_accuracy: 0.4785\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4909 - accuracy: 0.4637 - val_loss: 1.4770 - val_accuracy: 0.4727\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4870 - accuracy: 0.4664 - val_loss: 1.5058 - val_accuracy: 0.4633\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4847 - accuracy: 0.4676 - val_loss: 1.4624 - val_accuracy: 0.4734\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4827 - accuracy: 0.4685 - val_loss: 1.4693 - val_accuracy: 0.4727\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4783 - accuracy: 0.4693 - val_loss: 1.4809 - val_accuracy: 0.4743\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4719 - accuracy: 0.4711 - val_loss: 1.4674 - val_accuracy: 0.4761\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4727 - accuracy: 0.4719 - val_loss: 1.4593 - val_accuracy: 0.4784\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4705 - accuracy: 0.4713 - val_loss: 1.4635 - val_accuracy: 0.4756\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4728 - accuracy: 0.4679 - val_loss: 1.4634 - val_accuracy: 0.4767\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4695 - accuracy: 0.4706 - val_loss: 1.4809 - val_accuracy: 0.4737\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4664 - accuracy: 0.4751 - val_loss: 1.4658 - val_accuracy: 0.4716\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4657 - accuracy: 0.4738 - val_loss: 1.4557 - val_accuracy: 0.4768\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4678 - accuracy: 0.4724 - val_loss: 1.4580 - val_accuracy: 0.4800\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4722 - accuracy: 0.4711 - val_loss: 1.4453 - val_accuracy: 0.4822\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4691 - accuracy: 0.4729 - val_loss: 1.4569 - val_accuracy: 0.4760\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4661 - accuracy: 0.4739 - val_loss: 1.4542 - val_accuracy: 0.4827\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4596 - accuracy: 0.4760 - val_loss: 1.4361 - val_accuracy: 0.4873\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4565 - accuracy: 0.4775 - val_loss: 1.4624 - val_accuracy: 0.4744\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4679 - accuracy: 0.4691 - val_loss: 1.4577 - val_accuracy: 0.4807\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4582 - accuracy: 0.4736 - val_loss: 1.4542 - val_accuracy: 0.4764\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4626 - accuracy: 0.4755 - val_loss: 1.4495 - val_accuracy: 0.4773\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4576 - accuracy: 0.4767 - val_loss: 1.4340 - val_accuracy: 0.4856\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4479 - accuracy: 0.4794 - val_loss: 1.4517 - val_accuracy: 0.4880\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4546 - accuracy: 0.4797 - val_loss: 1.4412 - val_accuracy: 0.4828\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4596 - accuracy: 0.4765 - val_loss: 1.4587 - val_accuracy: 0.4802\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4649 - accuracy: 0.4730 - val_loss: 1.4505 - val_accuracy: 0.4800\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4486 - accuracy: 0.4804 - val_loss: 1.4486 - val_accuracy: 0.4845\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4516 - accuracy: 0.4782 - val_loss: 1.4370 - val_accuracy: 0.4861\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4588 - accuracy: 0.4754 - val_loss: 1.4391 - val_accuracy: 0.4861\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4479 - accuracy: 0.4815 - val_loss: 1.4352 - val_accuracy: 0.4843\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4490 - accuracy: 0.4791 - val_loss: 1.4458 - val_accuracy: 0.4832\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4400 - accuracy: 0.4832 - val_loss: 1.4325 - val_accuracy: 0.4898\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4442 - accuracy: 0.4823 - val_loss: 1.4398 - val_accuracy: 0.4809\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4357 - accuracy: 0.4844 - val_loss: 1.4631 - val_accuracy: 0.4778\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4367 - accuracy: 0.4866 - val_loss: 1.4542 - val_accuracy: 0.4760\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4350 - accuracy: 0.4844 - val_loss: 1.4355 - val_accuracy: 0.4874\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4321 - accuracy: 0.4861 - val_loss: 1.4271 - val_accuracy: 0.4902\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4319 - accuracy: 0.4859 - val_loss: 1.4461 - val_accuracy: 0.4827\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4375 - accuracy: 0.4840 - val_loss: 1.4319 - val_accuracy: 0.4912\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4465 - accuracy: 0.4801 - val_loss: 1.4339 - val_accuracy: 0.4978\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4420 - accuracy: 0.4844 - val_loss: 1.4304 - val_accuracy: 0.4939\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4391 - accuracy: 0.4848 - val_loss: 1.4610 - val_accuracy: 0.4693\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4327 - accuracy: 0.4867 - val_loss: 1.4252 - val_accuracy: 0.4933\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4352 - accuracy: 0.4827 - val_loss: 1.4349 - val_accuracy: 0.4878\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4387 - accuracy: 0.4841 - val_loss: 1.4541 - val_accuracy: 0.4795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3865a2880>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cut and paste your code from Section 3.2 below, then modify it to get\n",
    "much better results than what you had earlier. E.g. increase the number of\n",
    "nodes in the hidden layer, increase the number of hidden layers,\n",
    "change the optimizer, etc. \n",
    "\n",
    "Train for 100 epochs.\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "model2 = models.Sequential([\n",
    "    layers.Input(shape=(3072,)),\n",
    "    layers.Dense(2048, activation='sigmoid'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='sigmoid'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='sigmoid'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model2.fit(train_x, train_y, epochs=100, batch_size=64, validation_data=(test_x, test_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 100 epoches: \n",
    "\n",
    "Final training accuracy: 0.4841\n",
    "\n",
    "Final validation accuracy: 0.4795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we train for 150 epoches for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  5/782 [..............................] - ETA: 9s - loss: 2.8139 - accuracy: 0.0844  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 23:14:43.475272: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 0s - loss: 1.9893 - accuracy: 0.2603"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 23:14:52.213908: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 12ms/step - loss: 1.9893 - accuracy: 0.2603 - val_loss: 1.8642 - val_accuracy: 0.3178\n",
      "Epoch 2/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.8352 - accuracy: 0.3302 - val_loss: 1.7835 - val_accuracy: 0.3494\n",
      "Epoch 3/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.7920 - accuracy: 0.3483 - val_loss: 1.7543 - val_accuracy: 0.3703\n",
      "Epoch 4/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.7572 - accuracy: 0.3632 - val_loss: 1.7012 - val_accuracy: 0.3894\n",
      "Epoch 5/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.7435 - accuracy: 0.3679 - val_loss: 1.6729 - val_accuracy: 0.3927\n",
      "Epoch 6/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.7201 - accuracy: 0.3781 - val_loss: 1.6536 - val_accuracy: 0.3981\n",
      "Epoch 7/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.7070 - accuracy: 0.3832 - val_loss: 1.6603 - val_accuracy: 0.3962\n",
      "Epoch 8/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.7010 - accuracy: 0.3864 - val_loss: 1.7211 - val_accuracy: 0.3839\n",
      "Epoch 9/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.6897 - accuracy: 0.3906 - val_loss: 1.6192 - val_accuracy: 0.4137\n",
      "Epoch 10/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.6824 - accuracy: 0.3942 - val_loss: 1.6005 - val_accuracy: 0.4236\n",
      "Epoch 11/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.6733 - accuracy: 0.3999 - val_loss: 1.6236 - val_accuracy: 0.4098\n",
      "Epoch 12/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.6644 - accuracy: 0.3987 - val_loss: 1.5777 - val_accuracy: 0.4319\n",
      "Epoch 13/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.6518 - accuracy: 0.4057 - val_loss: 1.6291 - val_accuracy: 0.4184\n",
      "Epoch 14/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.6451 - accuracy: 0.4057 - val_loss: 1.5945 - val_accuracy: 0.4119\n",
      "Epoch 15/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.6430 - accuracy: 0.4071 - val_loss: 1.5706 - val_accuracy: 0.4302\n",
      "Epoch 16/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.6313 - accuracy: 0.4120 - val_loss: 1.5800 - val_accuracy: 0.4245\n",
      "Epoch 17/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.6318 - accuracy: 0.4130 - val_loss: 1.6165 - val_accuracy: 0.4164\n",
      "Epoch 18/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.6311 - accuracy: 0.4128 - val_loss: 1.5591 - val_accuracy: 0.4336\n",
      "Epoch 19/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.6253 - accuracy: 0.4132 - val_loss: 1.5655 - val_accuracy: 0.4357\n",
      "Epoch 20/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.6148 - accuracy: 0.4204 - val_loss: 1.5727 - val_accuracy: 0.4241\n",
      "Epoch 21/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.6076 - accuracy: 0.4231 - val_loss: 1.5686 - val_accuracy: 0.4374\n",
      "Epoch 22/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.6010 - accuracy: 0.4249 - val_loss: 1.5587 - val_accuracy: 0.4343\n",
      "Epoch 23/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.6009 - accuracy: 0.4229 - val_loss: 1.5550 - val_accuracy: 0.4365\n",
      "Epoch 24/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5968 - accuracy: 0.4231 - val_loss: 1.5446 - val_accuracy: 0.4402\n",
      "Epoch 25/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5861 - accuracy: 0.4296 - val_loss: 1.5586 - val_accuracy: 0.4392\n",
      "Epoch 26/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5824 - accuracy: 0.4352 - val_loss: 1.5331 - val_accuracy: 0.4482\n",
      "Epoch 27/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5806 - accuracy: 0.4296 - val_loss: 1.5305 - val_accuracy: 0.4474\n",
      "Epoch 28/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5790 - accuracy: 0.4336 - val_loss: 1.5446 - val_accuracy: 0.4439\n",
      "Epoch 29/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.5735 - accuracy: 0.4336 - val_loss: 1.5476 - val_accuracy: 0.4467\n",
      "Epoch 30/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.5702 - accuracy: 0.4354 - val_loss: 1.5081 - val_accuracy: 0.4583\n",
      "Epoch 31/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5649 - accuracy: 0.4373 - val_loss: 1.5298 - val_accuracy: 0.4469\n",
      "Epoch 32/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5630 - accuracy: 0.4372 - val_loss: 1.5011 - val_accuracy: 0.4588\n",
      "Epoch 33/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5633 - accuracy: 0.4386 - val_loss: 1.5310 - val_accuracy: 0.4554\n",
      "Epoch 34/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5602 - accuracy: 0.4380 - val_loss: 1.5205 - val_accuracy: 0.4474\n",
      "Epoch 35/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5562 - accuracy: 0.4410 - val_loss: 1.5019 - val_accuracy: 0.4582\n",
      "Epoch 36/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5546 - accuracy: 0.4432 - val_loss: 1.5151 - val_accuracy: 0.4528\n",
      "Epoch 37/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5565 - accuracy: 0.4432 - val_loss: 1.5026 - val_accuracy: 0.4544\n",
      "Epoch 38/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5477 - accuracy: 0.4474 - val_loss: 1.5262 - val_accuracy: 0.4530\n",
      "Epoch 39/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5498 - accuracy: 0.4420 - val_loss: 1.4992 - val_accuracy: 0.4592\n",
      "Epoch 40/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5452 - accuracy: 0.4442 - val_loss: 1.4924 - val_accuracy: 0.4637\n",
      "Epoch 41/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5472 - accuracy: 0.4450 - val_loss: 1.4991 - val_accuracy: 0.4605\n",
      "Epoch 42/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5409 - accuracy: 0.4466 - val_loss: 1.4947 - val_accuracy: 0.4585\n",
      "Epoch 43/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5381 - accuracy: 0.4454 - val_loss: 1.5265 - val_accuracy: 0.4507\n",
      "Epoch 44/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5406 - accuracy: 0.4470 - val_loss: 1.5023 - val_accuracy: 0.4546\n",
      "Epoch 45/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5368 - accuracy: 0.4474 - val_loss: 1.4831 - val_accuracy: 0.4641\n",
      "Epoch 46/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5329 - accuracy: 0.4490 - val_loss: 1.4910 - val_accuracy: 0.4657\n",
      "Epoch 47/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5272 - accuracy: 0.4497 - val_loss: 1.5039 - val_accuracy: 0.4548\n",
      "Epoch 48/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5326 - accuracy: 0.4481 - val_loss: 1.5125 - val_accuracy: 0.4514\n",
      "Epoch 49/150\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5303 - accuracy: 0.4512 - val_loss: 1.4909 - val_accuracy: 0.4616\n",
      "Epoch 50/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5206 - accuracy: 0.4514 - val_loss: 1.4882 - val_accuracy: 0.4619\n",
      "Epoch 51/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5227 - accuracy: 0.4522 - val_loss: 1.4857 - val_accuracy: 0.4667\n",
      "Epoch 52/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.5172 - accuracy: 0.4540 - val_loss: 1.4954 - val_accuracy: 0.4653\n",
      "Epoch 53/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.5159 - accuracy: 0.4546 - val_loss: 1.4927 - val_accuracy: 0.4594\n",
      "Epoch 54/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.5214 - accuracy: 0.4529 - val_loss: 1.4804 - val_accuracy: 0.4642\n",
      "Epoch 55/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5193 - accuracy: 0.4540 - val_loss: 1.4872 - val_accuracy: 0.4687\n",
      "Epoch 56/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.5168 - accuracy: 0.4544 - val_loss: 1.4871 - val_accuracy: 0.4648\n",
      "Epoch 57/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5200 - accuracy: 0.4527 - val_loss: 1.4795 - val_accuracy: 0.4706\n",
      "Epoch 58/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5116 - accuracy: 0.4563 - val_loss: 1.4732 - val_accuracy: 0.4733\n",
      "Epoch 59/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5099 - accuracy: 0.4600 - val_loss: 1.5068 - val_accuracy: 0.4540\n",
      "Epoch 60/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5081 - accuracy: 0.4593 - val_loss: 1.4969 - val_accuracy: 0.4645\n",
      "Epoch 61/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5013 - accuracy: 0.4602 - val_loss: 1.4737 - val_accuracy: 0.4668\n",
      "Epoch 62/150\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5031 - accuracy: 0.4590 - val_loss: 1.4702 - val_accuracy: 0.4690\n",
      "Epoch 63/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5047 - accuracy: 0.4581 - val_loss: 1.4891 - val_accuracy: 0.4586\n",
      "Epoch 64/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.5049 - accuracy: 0.4606 - val_loss: 1.5023 - val_accuracy: 0.4535\n",
      "Epoch 65/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4977 - accuracy: 0.4604 - val_loss: 1.4898 - val_accuracy: 0.4609\n",
      "Epoch 66/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4857 - accuracy: 0.4660 - val_loss: 1.4869 - val_accuracy: 0.4620\n",
      "Epoch 67/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4953 - accuracy: 0.4629 - val_loss: 1.4638 - val_accuracy: 0.4731\n",
      "Epoch 68/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5016 - accuracy: 0.4625 - val_loss: 1.4925 - val_accuracy: 0.4646\n",
      "Epoch 69/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4961 - accuracy: 0.4631 - val_loss: 1.4737 - val_accuracy: 0.4702\n",
      "Epoch 70/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4974 - accuracy: 0.4598 - val_loss: 1.4732 - val_accuracy: 0.4677\n",
      "Epoch 71/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4891 - accuracy: 0.4602 - val_loss: 1.4907 - val_accuracy: 0.4651\n",
      "Epoch 72/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4901 - accuracy: 0.4632 - val_loss: 1.4758 - val_accuracy: 0.4695\n",
      "Epoch 73/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4921 - accuracy: 0.4648 - val_loss: 1.4763 - val_accuracy: 0.4693\n",
      "Epoch 74/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4948 - accuracy: 0.4619 - val_loss: 1.5194 - val_accuracy: 0.4581\n",
      "Epoch 75/150\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4881 - accuracy: 0.4658 - val_loss: 1.4566 - val_accuracy: 0.4751\n",
      "Epoch 76/150\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4792 - accuracy: 0.4667 - val_loss: 1.4624 - val_accuracy: 0.4783\n",
      "Epoch 77/150\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4832 - accuracy: 0.4662 - val_loss: 1.4801 - val_accuracy: 0.4694\n",
      "Epoch 78/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4816 - accuracy: 0.4667 - val_loss: 1.4796 - val_accuracy: 0.4689\n",
      "Epoch 79/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4883 - accuracy: 0.4658 - val_loss: 1.4864 - val_accuracy: 0.4663\n",
      "Epoch 80/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4846 - accuracy: 0.4666 - val_loss: 1.4642 - val_accuracy: 0.4729\n",
      "Epoch 81/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4812 - accuracy: 0.4681 - val_loss: 1.4779 - val_accuracy: 0.4691\n",
      "Epoch 82/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4810 - accuracy: 0.4694 - val_loss: 1.4818 - val_accuracy: 0.4683\n",
      "Epoch 83/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4767 - accuracy: 0.4686 - val_loss: 1.4705 - val_accuracy: 0.4699\n",
      "Epoch 84/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4790 - accuracy: 0.4699 - val_loss: 1.4680 - val_accuracy: 0.4705\n",
      "Epoch 85/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4729 - accuracy: 0.4761 - val_loss: 1.4717 - val_accuracy: 0.4720\n",
      "Epoch 86/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4710 - accuracy: 0.4709 - val_loss: 1.4718 - val_accuracy: 0.4689\n",
      "Epoch 87/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4744 - accuracy: 0.4724 - val_loss: 1.4704 - val_accuracy: 0.4721\n",
      "Epoch 88/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4740 - accuracy: 0.4695 - val_loss: 1.4547 - val_accuracy: 0.4768\n",
      "Epoch 89/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4642 - accuracy: 0.4740 - val_loss: 1.4617 - val_accuracy: 0.4766\n",
      "Epoch 90/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4701 - accuracy: 0.4736 - val_loss: 1.4718 - val_accuracy: 0.4709\n",
      "Epoch 91/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4705 - accuracy: 0.4739 - val_loss: 1.4826 - val_accuracy: 0.4643\n",
      "Epoch 92/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4653 - accuracy: 0.4759 - val_loss: 1.4656 - val_accuracy: 0.4721\n",
      "Epoch 93/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4653 - accuracy: 0.4703 - val_loss: 1.4649 - val_accuracy: 0.4743\n",
      "Epoch 94/150\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4629 - accuracy: 0.4733 - val_loss: 1.4845 - val_accuracy: 0.4698\n",
      "Epoch 95/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4637 - accuracy: 0.4746 - val_loss: 1.4513 - val_accuracy: 0.4824\n",
      "Epoch 96/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4647 - accuracy: 0.4717 - val_loss: 1.4632 - val_accuracy: 0.4742\n",
      "Epoch 97/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4475 - accuracy: 0.4798 - val_loss: 1.4610 - val_accuracy: 0.4773\n",
      "Epoch 98/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4487 - accuracy: 0.4784 - val_loss: 1.4929 - val_accuracy: 0.4651\n",
      "Epoch 99/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4440 - accuracy: 0.4806 - val_loss: 1.4505 - val_accuracy: 0.4767\n",
      "Epoch 100/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4537 - accuracy: 0.4800 - val_loss: 1.4563 - val_accuracy: 0.4760\n",
      "Epoch 101/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4480 - accuracy: 0.4776 - val_loss: 1.4406 - val_accuracy: 0.4834\n",
      "Epoch 102/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4514 - accuracy: 0.4792 - val_loss: 1.4507 - val_accuracy: 0.4800\n",
      "Epoch 103/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4515 - accuracy: 0.4792 - val_loss: 1.4798 - val_accuracy: 0.4713\n",
      "Epoch 104/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4525 - accuracy: 0.4793 - val_loss: 1.4594 - val_accuracy: 0.4741\n",
      "Epoch 105/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4493 - accuracy: 0.4797 - val_loss: 1.4553 - val_accuracy: 0.4752\n",
      "Epoch 106/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4514 - accuracy: 0.4781 - val_loss: 1.4544 - val_accuracy: 0.4784\n",
      "Epoch 107/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4513 - accuracy: 0.4789 - val_loss: 1.4607 - val_accuracy: 0.4737\n",
      "Epoch 108/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4413 - accuracy: 0.4838 - val_loss: 1.4546 - val_accuracy: 0.4829\n",
      "Epoch 109/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4496 - accuracy: 0.4787 - val_loss: 1.4459 - val_accuracy: 0.4819\n",
      "Epoch 110/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4592 - accuracy: 0.4740 - val_loss: 1.4562 - val_accuracy: 0.4778\n",
      "Epoch 111/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4640 - accuracy: 0.4747 - val_loss: 1.4551 - val_accuracy: 0.4811\n",
      "Epoch 112/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4533 - accuracy: 0.4781 - val_loss: 1.4602 - val_accuracy: 0.4785\n",
      "Epoch 113/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4476 - accuracy: 0.4809 - val_loss: 1.4633 - val_accuracy: 0.4783\n",
      "Epoch 114/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4463 - accuracy: 0.4810 - val_loss: 1.4600 - val_accuracy: 0.4717\n",
      "Epoch 115/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4406 - accuracy: 0.4846 - val_loss: 1.4565 - val_accuracy: 0.4730\n",
      "Epoch 116/150\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 1.4423 - accuracy: 0.4809 - val_loss: 1.4624 - val_accuracy: 0.4741\n",
      "Epoch 117/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4388 - accuracy: 0.4839 - val_loss: 1.4450 - val_accuracy: 0.4836\n",
      "Epoch 118/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4412 - accuracy: 0.4838 - val_loss: 1.4554 - val_accuracy: 0.4792\n",
      "Epoch 119/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4334 - accuracy: 0.4838 - val_loss: 1.4589 - val_accuracy: 0.4702\n",
      "Epoch 120/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4425 - accuracy: 0.4843 - val_loss: 1.4482 - val_accuracy: 0.4782\n",
      "Epoch 121/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4358 - accuracy: 0.4832 - val_loss: 1.4492 - val_accuracy: 0.4780\n",
      "Epoch 122/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4426 - accuracy: 0.4821 - val_loss: 1.4717 - val_accuracy: 0.4764\n",
      "Epoch 123/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4447 - accuracy: 0.4797 - val_loss: 1.4467 - val_accuracy: 0.4849\n",
      "Epoch 124/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4476 - accuracy: 0.4776 - val_loss: 1.4718 - val_accuracy: 0.4692\n",
      "Epoch 125/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4370 - accuracy: 0.4837 - val_loss: 1.4359 - val_accuracy: 0.4820\n",
      "Epoch 126/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4319 - accuracy: 0.4860 - val_loss: 1.4363 - val_accuracy: 0.4821\n",
      "Epoch 127/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4341 - accuracy: 0.4850 - val_loss: 1.4516 - val_accuracy: 0.4804\n",
      "Epoch 128/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4384 - accuracy: 0.4827 - val_loss: 1.4414 - val_accuracy: 0.4796\n",
      "Epoch 129/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4343 - accuracy: 0.4825 - val_loss: 1.4515 - val_accuracy: 0.4833\n",
      "Epoch 130/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4302 - accuracy: 0.4871 - val_loss: 1.4434 - val_accuracy: 0.4816\n",
      "Epoch 131/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4297 - accuracy: 0.4847 - val_loss: 1.4579 - val_accuracy: 0.4730\n",
      "Epoch 132/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4263 - accuracy: 0.4857 - val_loss: 1.4433 - val_accuracy: 0.4834\n",
      "Epoch 133/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4251 - accuracy: 0.4880 - val_loss: 1.4632 - val_accuracy: 0.4778\n",
      "Epoch 134/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4268 - accuracy: 0.4870 - val_loss: 1.4461 - val_accuracy: 0.4827\n",
      "Epoch 135/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4244 - accuracy: 0.4876 - val_loss: 1.4530 - val_accuracy: 0.4775\n",
      "Epoch 136/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4309 - accuracy: 0.4834 - val_loss: 1.4390 - val_accuracy: 0.4830\n",
      "Epoch 137/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4319 - accuracy: 0.4834 - val_loss: 1.4587 - val_accuracy: 0.4791\n",
      "Epoch 138/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4296 - accuracy: 0.4845 - val_loss: 1.4486 - val_accuracy: 0.4831\n",
      "Epoch 139/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4331 - accuracy: 0.4849 - val_loss: 1.4468 - val_accuracy: 0.4818\n",
      "Epoch 140/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4264 - accuracy: 0.4851 - val_loss: 1.4765 - val_accuracy: 0.4746\n",
      "Epoch 141/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4339 - accuracy: 0.4841 - val_loss: 1.4515 - val_accuracy: 0.4792\n",
      "Epoch 142/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4332 - accuracy: 0.4836 - val_loss: 1.4486 - val_accuracy: 0.4825\n",
      "Epoch 143/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4280 - accuracy: 0.4872 - val_loss: 1.4549 - val_accuracy: 0.4787\n",
      "Epoch 144/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4264 - accuracy: 0.4869 - val_loss: 1.4501 - val_accuracy: 0.4783\n",
      "Epoch 145/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4281 - accuracy: 0.4862 - val_loss: 1.4573 - val_accuracy: 0.4755\n",
      "Epoch 146/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4298 - accuracy: 0.4881 - val_loss: 1.4494 - val_accuracy: 0.4836\n",
      "Epoch 147/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4282 - accuracy: 0.4895 - val_loss: 1.4479 - val_accuracy: 0.4875\n",
      "Epoch 148/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4268 - accuracy: 0.4882 - val_loss: 1.4479 - val_accuracy: 0.4785\n",
      "Epoch 149/150\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 1.4237 - accuracy: 0.4886 - val_loss: 1.4575 - val_accuracy: 0.4780\n",
      "Epoch 150/150\n",
      "782/782 [==============================] - 8s 11ms/step - loss: 1.4246 - accuracy: 0.4874 - val_loss: 1.4451 - val_accuracy: 0.4830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3b22a1310>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we continue to try to run for 150 epoches\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "model2 = models.Sequential([\n",
    "    layers.Input(shape=(3072,)),\n",
    "    layers.Dense(2048, activation='sigmoid'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1024, activation='sigmoid'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(512, activation='sigmoid'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model2.fit(train_x, train_y, epochs=150, batch_size=64, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 4:\n",
    "\n",
    "Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the learning_rate, momentum etc rows with parameters more appropriate to the optimizer that you have chosen.\n",
    "\n",
    "| Hyperparameter         | What I used | Why?                                                                 |\n",
    "|:-----------------------|:------------|:----------------------------------------------------------------------|\n",
    "| Optimizer              | Adam        | Combines momentum and adaptive learning rate; performs well in practice |\n",
    "| # of hidden layers     | 3           | Allows model to learn complex hierarchical features                    |\n",
    "| # neurons (layer1)     | 2048        | Large layer to capture high-dimensional input features                |\n",
    "| Hid layer1 activation  | Sigmoid     | Performed better than ReLU in this specific task after trials         |\n",
    "| Dropout (after layer1) | 0.2         | Prevents overfitting by randomly deactivating neurons during training |\n",
    "| # neurons (layer2)     | 1024        | Reduces dimensionality while preserving learned patterns              |\n",
    "| Hid layer2 activation  | Sigmoid     | Keeps activation consistent and effective                             |\n",
    "| Dropout (after layer2) | 0.2         | Same reason as above                                                  |\n",
    "| # neurons (layer3)     | 512         | Further abstraction, preparing for output layer                       |\n",
    "| Hid layer3 activation  | Sigmoid     | Same reason as above                                                  |\n",
    "| Dropout (after layer3) | 0.2         | Same reason as above                                                  |\n",
    "| # of output neurons    | 10          | CIFAR-10 has 10 classes                                               |\n",
    "| Output activation      | Softmax     | Converts logits to class probabilities                                |\n",
    "| learning_rate          | 0.001       | Default and stable value for Adam optimizer                           |\n",
    "| momentum               | N/A         | Not used with Adam                                                    |\n",
    "| loss                   | Categorical crossentropy | Suitable for one-hot multi-class classification              |\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "What is the final training and validation accuracy that you obtained after 150 epochs. Is there considerable improvement over Section 3.2? Are there still signs of underfitting or overfitting? Explain your answer.\n",
    "\n",
    "***Write your answers here***\n",
    "\n",
    "For 150 epoches:\n",
    "\n",
    "Final training accuracy: 0.4874\n",
    "\n",
    "Final validation accuracy: 0.4830\n",
    "\n",
    "There is a very slight improvement compared to the initial MLP model, with the accuracy from 0.4652 incresing to 0.4874.\n",
    "\n",
    "Still signs of underfitting, the gap between training and validation accuracy is not very large, but both accuracies are still not very high. This suggests that the model still hasn’t fully captured the complexity of the CIFAR-10 data.\n",
    "\n",
    "#### Question 6\n",
    "\n",
    "Write a short reflection on the practical difficulties of using a dense MLP to classsify images in the CIFAR-10 datasets.\n",
    "\n",
    "***Write your answers here***\n",
    "\n",
    "Using a dense MLP for CIFAR-10 is challenging because flattening the image loses important spatial features. Without convolution, the model can’t easily detect patterns like edges or textures, leading to low accuracy even after long training.\n",
    "\n",
    "----\n",
    "\n",
    "## 4. Creating a CNN for the MNIST Data Set\n",
    "\n",
    "In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in the previous lab. Let's go through each part to see how to do this.\n",
    "\n",
    "### 4.1 Loading the MNIST Dataset\n",
    "\n",
    "As always we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def load_mnist():\n",
    "    (train_x, train_y),(test_x, test_y) = mnist.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
    "    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
    "\n",
    "    train_x=train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    \n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "        \n",
    "    train_y = to_categorical(train_y, 10)\n",
    "    test_y = to_categorical(test_y, 10)\n",
    "        \n",
    "    return (train_x, train_y), (test_x, test_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the CNN\n",
    "\n",
    "We will now build the CNN. Unlike before we will create a function to produce the CNN. We will also look at how to save and load Keras models using \"checkpoints\", particularly \"ModelCheckpoint\" that saves the model each epoch.\n",
    "\n",
    "Let's begin by creating the model. We call os.path.exists to see if a model file exists, and call \"load_model\" if it does. Otherwise we create a new model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model loads a model from a hd5 file.\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "MODEL_NAME = 'mnist-cnn.keras'\n",
    "\n",
    "def buildmodel(model_name):\n",
    "    if os.path.exists(model_name):\n",
    "        model = load_model(model_name)                                                                                             \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(5,5),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
    "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "        model.add(Flatten()) # Question 9\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 7\n",
    "\n",
    "The first layer in our CNN is a 2D convolution kernel, shown here:\n",
    "\n",
    "```\n",
    "        model.add(Conv2D(32, kernel_size=(5,5),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
    "```\n",
    "\n",
    "Why is the input_shape set to (28, 28, 1)? What does this mean? What does \"padding = 'same'\" mean? \n",
    "\n",
    "***Write your answer here***\n",
    "\n",
    "The reason why input_shape is set to (28,28,1) is that this matches the format of MNIST images.\n",
    "\n",
    "input_shape=(28, 28, 1) means the input images are 28x28 pixels, and they are 1 channel—standard for grayscale MNIST images.\n",
    "\n",
    "padding='same' means that the output of the convolution layer has the same spatial dimensions as the input (28x28), by adding zero-padding around the edges. \n",
    "\n",
    "#### Question 8\n",
    "\n",
    "The second layer is the MaxPooling2D layer shown below:\n",
    "\n",
    "```\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
    "```\n",
    "\n",
    "What other types of pooling layers are available? What does 'strides = 2' mean? \n",
    "\n",
    "***Write your answer here***\n",
    "\n",
    "Other pooling layers available:\n",
    "\n",
    "AveragePooling2D: takes the average value in each pooling window;\n",
    "\n",
    "GlobalMaxPooling2D / GlobalAveragePooling2D: performs pooling over the entire feature map.\n",
    "\n",
    "strides=2 means the pooling window moves 2 pixels at a time.\n",
    "\n",
    "#### Question 9\n",
    "\n",
    "What does the \"Flatten\" layer here do? Why is it needed?\n",
    "\n",
    "```\n",
    "        model.add(Flatten()) # Question 9\n",
    "```\n",
    "\n",
    "***Write your answer here***\n",
    "\n",
    "The Flatten() layer converts the multi-dimensional feature maps into a 1D vector. This helps connect the convolutional part of the model to the fully connected layers for classification.\n",
    "\n",
    "----\n",
    "\n",
    "### 4.3 Training the CNN\n",
    "\n",
    "Let's now train the CNN. In this example we introduce the idea of a \"callback\", which is a routine that Keras calls at the end of each epoch. Specifically we look at two callbacks:\n",
    "\n",
    "    1. ModelCheckpoint: When called, Keras saves the model to the specified filename.\n",
    "    \n",
    "    2. EarlyStopping: When called, Keras checks if it should stop the training prematurely.\n",
    "    \n",
    "\n",
    "Let's look at the code to see how training is done, and how callbacks are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
    "\n",
    "    model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.7), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    savemodel = ModelCheckpoint(model_name)\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "\n",
    "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
    "    validation_data=(test_x, test_y), shuffle=True,\n",
    "    epochs=epochs, \n",
    "    callbacks=[savemodel, stopmodel])\n",
    "\n",
    "    print(\"Done. Now evaluating.\")\n",
    "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
    "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there isn't very much that is unusual going on; we compile the model with our loss function and optimizer, then call fit, and finally evaluate to look at the final accuracy for the test set.  The only thing unusual is the \"callbacks\" parameter here in the fit function call\n",
    "\n",
    "```\n",
    "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
    "    validation_data=(test_x, test_y), shuffle=True,\n",
    "    epochs=epochs, \n",
    "    callbacks=[savemodel, stopmodel])\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 10.\n",
    "\n",
    "What does do the min_delta and patience parameters do in the EarlyStopping callback, as shown below? (2 MARKS)\n",
    "\n",
    "```\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
    "```\n",
    "\n",
    "min_delta=0.001 sets the minimum change in the monitored metric to qualify as an improvement.\n",
    "\n",
    "patience=10 allows training to continue for 10 more epochs without significant improvement before stopping early.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Putting it together.\n",
    "\n",
    "Now let's run the code and see how it goes (Note: To save time we are training for only 5 epochs; we should train much longer to get much better results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 3s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 00:28:53.619064: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8872"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 00:29:11.022251: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3395 - accuracy: 0.8874 - val_loss: 0.0863 - val_accuracy: 0.9738\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0756 - accuracy: 0.9771 - val_loss: 0.0522 - val_accuracy: 0.9833\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0505 - accuracy: 0.9844 - val_loss: 0.0400 - val_accuracy: 0.9860\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0381 - accuracy: 0.9880 - val_loss: 0.0475 - val_accuracy: 0.9855\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.0335 - val_accuracy: 0.9892\n",
      "Done. Now evaluating.\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0335 - accuracy: 0.9892\n",
      "Test accuracy: 0.99, loss: 0.03\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y),(test_x, test_y) = load_mnist()\n",
    "model = buildmodel(MODEL_NAME)\n",
    "train(model, train_x, train_y, 5, test_x, test_y, MODEL_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 11.\n",
    "\n",
    "Compare the relative advantages and disadvantages of CNN vs. the Dense MLP that you build in sections 3.2 and 3.3. What makes CNNs better (or worse)?\n",
    "\n",
    "***Type your answers here***\n",
    "\n",
    "CNNs work better than MLPs for image tasks like CIFAR-10 because they keep the spatial structure of the image and can detect local patterns like edges. They also use fewer parameters thanks to weight sharing, which makes training more efficient.\n",
    "\n",
    "In contrast, MLPs flatten the image and treat all pixels the same, which loses important spatial information. That’s why they usually perform worse on image data.\n",
    "\n",
    "The downside of CNNs is that they can be more complex to design and train, but the boost in accuracy is often worth it.\n",
    "\n",
    "## 5. Making a CNN for the CIFAR-10 Dataset\n",
    "\n",
    "Now comes the fun part: Using the example above for creating a CNN for the MNIST dataset, now create a CNN in the box below for the CIFAR-10 dataset. At the end of each epoch save the model to a file called \"cifar.hd5\" (note: the .hd5 is added automatically for you).\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 12.\n",
    "\n",
    "Summarize your design in the table below (the actual coding cell comes after this):\n",
    "\n",
    "| Hyperparameter       | What I used                    | Why?                              |\n",
    "|:---------------------|:-------------------------------|:----------------------------------|\n",
    "| Optimizer            | Adam                           | Fast convergence, works well      |\n",
    "| Input shape          | (32, 32, 3)                    | CIFAR-10 images are RGB 32×32     |\n",
    "| First layer          | Conv2D(64, 3×3, relu) + BN     | Extracts basic features           |\n",
    "| Second layer         | Conv2D(64, 3×3, relu) + BN + MaxPooling | Deepens feature extraction |\n",
    "| Add more layers      | Conv2D(128, 256) + BN + MaxPooling | Capture complex patterns    |\n",
    "| if needed            | BatchNormalization             | Stabilizes and speeds up training |\n",
    "| Dense layer          | Dense(1024→512→10) + Dropout   | Classifies and reduces overfitting |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write your code for your CNN for the CIFAR-10 dataset here. \n",
    "\n",
    "Note: train_x, train_y, test_x, test_y were changed when we called \n",
    "load_mnist in the previous section. We reload them below for you.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "train_x = train_x.astype('float32')\n",
    "test_x = test_x.astype('float32')\n",
    "train_x /= 255.0\n",
    "test_x /= 255.0\n",
    "ret_train_y = to_categorical(train_y,10)\n",
    "ret_test_y = to_categorical(test_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cifar'\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "def buildmodel(model_name):\n",
    "    if os.path.exists(model_name):\n",
    "        model = load_model(model_name)\n",
    "    else:\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu',\n",
    "                         input_shape=(32, 32, 3), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "        model.add(Conv2D(256, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(256, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    savemodel = ModelCheckpoint(model_name + '.h5')\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10)\n",
    "    model.fit(x=train_x, y=train_y, batch_size=64,\n",
    "              validation_data=(test_x, test_y), shuffle=True,\n",
    "              epochs=epochs, callbacks=[savemodel, stopmodel])\n",
    "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
    "    print(\"Test accuracy: %3.2f, loss: %3.2f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 03:02:36.214073: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 0s - loss: 8.6985 - accuracy: 0.2931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 03:02:53.214867: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 19s 22ms/step - loss: 8.6985 - accuracy: 0.2931 - val_loss: 3.3375 - val_accuracy: 0.5127\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 8.6699 - accuracy: 0.4083 - val_loss: 4.3945 - val_accuracy: 0.5727\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 7.3122 - accuracy: 0.4987 - val_loss: 3.4151 - val_accuracy: 0.6502\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 6.3213 - accuracy: 0.5597 - val_loss: 3.0519 - val_accuracy: 0.6932\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 5.3710 - accuracy: 0.6115 - val_loss: 2.7994 - val_accuracy: 0.7132\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 4.5410 - accuracy: 0.6489 - val_loss: 2.6985 - val_accuracy: 0.7294\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 3.8357 - accuracy: 0.6798 - val_loss: 2.7265 - val_accuracy: 0.7411\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 3.2368 - accuracy: 0.7106 - val_loss: 2.5905 - val_accuracy: 0.7341\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 2.7874 - accuracy: 0.7274 - val_loss: 2.0851 - val_accuracy: 0.7650\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 2.4236 - accuracy: 0.7441 - val_loss: 2.0516 - val_accuracy: 0.7634\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 2.3188 - accuracy: 0.7509 - val_loss: 2.2295 - val_accuracy: 0.7559\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 2.1531 - accuracy: 0.7658 - val_loss: 2.0068 - val_accuracy: 0.7674\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 1.9900 - accuracy: 0.7730 - val_loss: 1.8961 - val_accuracy: 0.7732\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 1.8824 - accuracy: 0.7805 - val_loss: 2.0232 - val_accuracy: 0.7614\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 1.6546 - accuracy: 0.7910 - val_loss: 2.3795 - val_accuracy: 0.7718\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 1.4890 - accuracy: 0.8014 - val_loss: 2.2541 - val_accuracy: 0.7696\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 1.4834 - accuracy: 0.8035 - val_loss: 2.0488 - val_accuracy: 0.7674\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 1.2763 - accuracy: 0.8140 - val_loss: 2.2359 - val_accuracy: 0.7816\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 1.1686 - accuracy: 0.8238 - val_loss: 2.2857 - val_accuracy: 0.7716\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 1.0958 - accuracy: 0.8345 - val_loss: 2.5685 - val_accuracy: 0.7737\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 1.0242 - accuracy: 0.8397 - val_loss: 2.7246 - val_accuracy: 0.7795\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 1.1474 - accuracy: 0.8352 - val_loss: 2.9064 - val_accuracy: 0.7921\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 1.1059 - accuracy: 0.8461 - val_loss: 2.8976 - val_accuracy: 0.7895\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.8976 - accuracy: 0.7895\n",
      "Test accuracy: 0.79, loss: 2.90\n"
     ]
    }
   ],
   "source": [
    "model = buildmodel(MODEL_NAME)\n",
    "train(model, train_x, ret_train_y, 50, test_x, ret_test_y, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final train accuracy: 0.8461\n",
    "\n",
    "Final validation accuracy: 0.7895\n",
    "\n",
    "Test accuracy: 0.79"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
